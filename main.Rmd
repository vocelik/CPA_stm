---
title: "Structural Topic Model - Government-Business Relationship in Academic Literature"
author: "Václav Ocelík"
date: "`r Sys.Date()`"
output: html_document
---

<style type="text/css">
.main-container {
  max-width: 1200px;
  margin-left: auto;
  margin-right: auto;
}
h1.title {
  font-size: 38px;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  text-align: center;
</style>

```{r setup, include=FALSE, message = F, echo = F}
knitr::opts_chunk$set(echo = T, message = F, error = F,
                      dpi = 300, fig.width = 6, fig.height = 4,
                      warning = F)
```

<body>

__The__ following is a structural topic model analysis of abstract from academic articles on the relationship between politics and business published between 1990 and 2020. The articles were acquired through _Thomson Reuters Web of Knowledge_. The following keywords search was used:  

<strong>TS = ( ("business", firm*, corporat*, compan*, enterpris*, entrepreneur*) AND (government*, bureaucra*, minist*, power, legislat*, judicia*, executiv*, elect*, public) ). </strong>  

This resulted in <strong>75.931 </strong> hits. I have thus far extracted the 30.000 most relevant (sorted by Web of Knowledge) Not all of these hits are relevant and/or suitable to the analysis, however. The dataset needs to be cleaned before it can be used in the structural topic model. 

The following libraries are required for the analysis.

```{r Libraries}
rm(list=ls())

library(furrr)
library(GGally)
library(ggthemes)
library(gridExtra)
library(knitr)
library(kableExtra)
library(lubridate)
library(network)
library(readxl)
library(tidylo)
library(tidystm)
library(tidytext)
library(tidyverse)
library(scales)
library(stm)
library(tm)

load("stm_sept_2020.RData")

theme_set(theme_classic())
```


We import the data iteratively, as it is divided over 50+ excel files. The folder containing the data should not include excel files other than the Web of Science data.

```{r Import and Combine Data}
# Import data ####
df <- do.call(rbind, lapply(Sys.glob("data/data_sept_2020/*.xls"), read_excel)) # make sure no other .xls files in target.
df
```

A list of all variables.

```{r}
relevant_cols <- df %>%
  janitor::clean_names() %>%
  select(abstract, publication_year) %>%
  names() 

relevant_row_nums <- df %>%
  janitor::clean_names() %>% 
  names() %>%
  as_tibble_col(column_name = "Variable") %>%
  filter(Variable != "x68") %>%
  mutate(row_num = row_number()) %>% 
  filter(Variable %in% relevant_cols) %>%
  pull(row_num)

df %>%
  janitor::clean_names() %>%
  names() %>%
  as_tibble_col(column_name = "Variable") %>%
  filter(Variable != "x68") %>%
  kbl(caption = "All variables from the data set provided by WOS.") %>%
  kable_classic("striped", full_width = F) %>%
  row_spec(relevant_row_nums, bold = T) %>%
  footnote(general = "Bold names are used in the structural topic model.",
           number = "The variable 'academic field' needs to be created.")

df %>%
  janitor::clean_names() %>%
  names() %>%
  as_tibble_col(column_name = "Variable") %>%
  filter(Variable != "x68") %>%
  htmlTable::htmlTable(n.cgroup = c(2,2),
                       rnames = F)


rm(relevant_cols, relevant_row_nums)
```

We can now select the columns that contain relevant information. 

```{r}
# keep only the necessary columns
df_clean <- df %>%
  janitor::clean_names() %>%
  select(authors, `article_title`, `source_title`, abstract,
         `publication_year`, `times_cited_all_databases`,
         `author_keywords`, `keywords_plus`, publication_date)

```

A small (N = 5) random sample from the data.

```{r}
df_clean %>%
  select(authors, `article_title`, `publication_year`, `times_cited_all_databases`,
         abstract) %>%
  drop_na() %>%
  sample_n(5) %>%
  kbl(caption = "A snapshot of the data") %>%
  kable_classic("striped", full_width = FALSE) %>%
  footnote(general = "Rows were randomly selected.")

```

Exploring the citation variable.

```{r eval = F}
df_clean %>%
  filter(`times_cited_all_databases` < 2000,
         !is.na(`publication_year`)) %>%
  ggplot(aes(x = `publication_year`, y = `times_cited_all_databases`)) +
  geom_point(aes(color = `times_cited_all_databases` > 250), show.legend = F)

df_clean %>%
  filter(`times_cited_all_databases` < 250,
         !is.na(`publication_year`)) %>%
  ggplot(aes(x = `publication_year`, y = `times_cited_all_databases`)) +
  geom_point(aes(color = `times_cited_all_databases` > 100), show.legend = F)

df_clean %>%
  filter(`times_cited_all_databases` < 100,
         !is.na(`publication_year`)) %>%
  ggplot(aes(x = `publication_year`, y = `times_cited_all_databases`)) +
  geom_point(aes(color = `times_cited_all_databases` > 25), show.legend = F)

```

Creating boxplots and histogram of citation count.

```{r}
cite_boxplot <- df_clean %>%
  ggplot(aes(times_cited_all_databases)) +
  geom_boxplot(outlier.alpha = .7) +
  coord_flip() +
  labs(x = NULL,
       y = "citations")

cite_boxplot_log <- df_clean %>%
  ggplot(aes(log(times_cited_all_databases))) +
  geom_boxplot(outlier.alpha = .7) +
  coord_flip() +
  labs(x = NULL,
       y = "log(citations)")

cite_hist <- df_clean %>%
  ggplot(aes(times_cited_all_databases)) +
  geom_histogram(color = "white") +
  labs(y = NULL,
       x = "citations")

cite_hist_log <- df_clean %>%
  ggplot(aes(log(times_cited_all_databases))) +
  geom_histogram(color = "white") +
  labs(y = NULL,
       x = "log(citations)")

grid.arrange(cite_boxplot, cite_boxplot_log,
             ncol = 2,
             top = "Boxplot comparison of citations and log(citations).",
             left = "times cited")

grid.arrange(cite_hist, cite_hist_log,
             ncol = 2,
             top = "Histogram comparison of citations and log(citations).",
             left = "Count",
             bottom = "Citation count")
rm(cite_boxplot, cite_boxplot_log, cite_hist, cite_hist_log)
```

As we can see, taking the natural logarithm mitigates the skewness somewhat, but long right-tails remain. 

---

```{r Additional Variables and Join}
# creating some additional variables
df_clean <- df_clean %>%
  filter(!is.na(`publication_year`),
         !is.na(abstract),
         !is.na(publication_date)) %>%
  mutate(ID = row_number(),
         abstract_Length = str_length(abstract),
         publication_date = case_when(publication_date == "JAN" ~ 1,
                                      publication_date == "FEB" ~ 2,
                                      publication_date == "MAR" ~ 3,
                                      publication_date == "APR" ~ 4,
                                      publication_date == "MAY" ~ 5,
                                      publication_date == "JUN" ~ 6,
                                      publication_date == "JUL" ~ 7,
                                      publication_date == "AUG" ~ 8,
                                      publication_date == "SEP" ~ 9,
                                      publication_date == "OCT" ~ 10,
                                      publication_date == "NOV" ~ 11,
                                      publication_date == "DEC" ~ 12)) %>%
  filter(!is.na(publication_date)) %>%
  unite("date", publication_year, publication_date, sep = "-") %>%
  mutate(date = paste(date, "01", sep = "-"), date = as.Date(date),
         months_since_first_pub = interval(min(date), date),
         months_since_first_pub = months_since_first_pub %/% months(1) + 1) 
```

Next, I import the journal data from the [Web of Science Master Journal List](https://mjl.clarivate.com/collection-list-downloads). I use the Social Sciences Citation Index (SSCI). Joining this dataframe on *df_clean* allows us to use the main field as metadata in the structural topic model. I will keep the 6 most common fields in the dataset.   

```{r}
df_wos <- read_csv("data/wos-core_SSCI.csv") %>%
  select(`Journal title`,`Web of Science Categories`) %>%
  rename(`source_title` = `Journal title`)

df_clean <- df_clean %>%
  left_join(distinct(df_wos)) 

df_clean <- df_clean %>%
  filter(!is.na(`Web of Science Categories`)) %>%
  mutate(field = str_remove(`Web of Science Categories`, ' [.]*[ |.].*'),
         field = str_remove(field, '[,].*'))

rm(df_wos)
```

What are the most common fields?

```{r}
df_clean %>%
  count(field, sort = T, name = "count") %>%
  top_n(6) %>%
  kbl(caption = "The most common fields to research the relationship between business and politics.") %>%
  kable_classic("striped", full_width = FALSE)
```

What are the most common journals?

```{r}
df_clean %>%
  count(source_title, sort = T, name = "count") %>%
  top_n(20) %>%
  mutate(source_title = str_to_title(source_title)) %>%
  kbl(caption = "The most common journals to publish research on the relationship between business and politics.") %>%
  kable_classic("striped", full_width = FALSE)
```

```{r}
df_clean %>%
  group_by(field, source_title) %>%
  summarise(most_pop = n()) %>%
  slice_max(most_pop, 5) %>%
  ungroup() %>%
  arrange(-most_pop) %>%
  mutate(source_title = str_to_title(source_title)) %>%
  kbl(caption = "The most common journals to publish research on the relationship between business and politics.") %>%
  kable_classic("striped", full_width = FALSE)
```

What are the most common keywords?

```{r}
df_clean %>%
  filter(!is.na(`keywords_plus`)) %>%
  separate(`keywords_plus`, sep = ";", into = "keywords") %>%
  mutate(keywords = tolower(keywords),
         keywords = str_replace_all(string = keywords, pattern = "-", replacement = " "),
         keywords = str_replace_all(string  = keywords, pattern = "([.*])", replacement = "")) %>%
  count(keywords, sort = T, name = "count") %>%
  head(20) %>%
  kbl(caption = "The most popular keywords") %>%
  kable_classic("striped", full_width = FALSE)
```

Exploring the most popular keywords per scientific field.

```{r}
df_clean %>%
  filter(!is.na(`keywords_plus`)) %>%
  separate(`keywords_plus`, sep = ";", into = "keywords") %>%
  mutate(keywords = tolower(keywords),
         keywords = str_replace_all(keywords, "-", " ")) %>%
  count(field, keywords, sort = T, name = "count") %>%
  head(25) %>%
  kbl(caption = "The most popular keywords by field.") %>%
  kable_classic("striped", full_width = FALSE) %>%
  footnote(general = "Keywords Plus provided by Web of Science.")
  

df_clean %>%
  filter(!is.na(`author_keywords`)) %>%
  separate(`author_keywords`, sep = ";", into = "keywords") %>%
  mutate(keywords = tolower(keywords),
         keywords = str_replace_all(keywords, "-", " "),
         keywords = str_replace_all(keywords, "[(].*[)]", " ")) %>%
  count(field, keywords, sort = T, name = "count") %>%
  head(25) %>%
  kbl(caption = "The most popular keywords by field.") %>%
  kable_classic("striped", full_width = FALSE) %>%
  footnote(general = "Keywords provided by authors.")
```

```{r}
fields <- df_clean %>%
  mutate(field = ifelse(field == "Business" | field == "Management", "business/management", field),
         field = ifelse(field == "International Relations" | field == "Public Administration", "Political Science", field),
         field = ifelse(field == "Communication" | field == "Sociology" | field == "Anthropology" | field == "Cultural Studies", "Social Sciences", field)) %>%
  count(field, sort = T) %>%
  top_n(6) %>%
  pull(field)
```

Prepare final table.

```{r}
df_clean_final <- df_clean %>%
  mutate(field = ifelse(field == "Business" | field == "Management", "business/management", field),
         field = ifelse(field == "International Relations" | field == "Public Administration", "Political Science", field),
         field = ifelse(field == "Communication" | field == "Sociology" | field == "Anthropology" | field == "Cultural Studies", "Social Sciences", field)) %>%
  filter(field %in% fields) %>%
  select(abstract, field, months_since_first_pub)

df_clean_final %>%
  sample_n(size = 5) %>%
  kbl(caption = "Final table with main meta data") %>%
  kable_classic("striped", full_width = F) 

```

The data set has now been reduced to 12393 articles. 

I will now first do some exploratory word-level analysis. This includes calculating the _tf_idf_ and the weighted log-odds of the terms. Stop words are obviously removed prior to the calculations. 

```{r Word-Level Analysis}

# extract the words out of the abstracts.
df_words <- df_clean_final %>%
  unnest_tokens(word, abstract) %>%
  anti_join(stop_words) 

custom_stopwords <- df_words %>%
  count(word, sort = T) %>%
  top_n(20) %>%
  pull(word)

# calculate tf_idf 
df_words_tf_idf <- df_words %>%
  count(word, field, sort = T) %>%
  bind_tf_idf(word, field, n) %>%
  arrange(-tf_idf) %>%
  group_by(field) %>%
  top_n(10) %>%
  ungroup()

# plot tf_idf
df_words_tf_idf %>%
  mutate(word = reorder_within(word, tf_idf, field)) %>%
  ggplot(aes(word, tf_idf, fill = field)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  scale_fill_grey() + 
  facet_wrap(~ field, scales = "free") +
  scale_x_reordered() +
  coord_flip() +
  labs(title = "TF-IDF words per Academic Field.")

# calculate log_odds
df_words_log_odds <- df_words %>%
  count(field, word) %>%
  bind_log_odds(field, word, n) %>%
  arrange(-log_odds_weighted)

# plot log_odds
df_words_log_odds %>%
  group_by(field) %>%
  slice_max(log_odds_weighted, n = 10) %>%
  mutate(word = reorder(word, log_odds_weighted)) %>%
  ungroup() %>%
  ggplot(aes(log_odds_weighted, word, fill = field)) +
  geom_col(alpha = 0.8, show.legend = F) +
  facet_wrap(~field, scales = "free") +
  scale_y_reordered() +
  labs(y = NULL,
       title = "Most propobable terms per Academic Field.") 

rm(df_words, df_words_log_odds, df_words_tf_idf)
```

Before proceeding with the topic model, I include some additional graphs to gain a deeper understanding of the structure of the dataset.

```{r Additional Graphs, eval = F}
# plot variance in citation count per field and per type. 
df_clean_final %>%
  ggplot(aes(reorder(field, times_cited_all_databases, median), times_cited_all_databases)) +
  geom_boxplot() +
  labs(y = NULL,
       x = NULL,
       title = "Boxplot of Average Citation Count per Field",
       subtitle = "With Type as subcategory."
       )

# plot histogram of publications per type. 
df_clean_final %>%
  ggplot(aes(`year_since_pub`)) +
  geom_histogram(color = "white", bins = 30) +
  scale_fill_economist() +
  labs(y = NULL,
       x = "Years since first publication.",
       title = "Histogram of publications on CPA.",
       subtitle = "From 1990 to Present")

# plot probability density of publications per type. 
df_clean_final %>%
  ggplot(aes(year_since_pub, ..density..)) +
  geom_freqpoly() +
  scale_color_wsj() +
  labs(y = NULL,
       x = NULL,
       title = "Probability density of publications on CPA.") 
```

Now, I proceed with the structural topic model. The following steps are based on the _stm_ vignette by [Roberts et al. (2014).](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf). I also apply some code written by [Julia Silge](https://juliasilge.com/blog/evaluating-stm/) which allows for helpful plots not available in the stm package.

```{r STM}
df_prep <- textProcessor(df_clean_final$abstract,
                         metadata = df_clean_final,
                         verbose = F,
                         onlycharacter = T,
                         customstopwords = custom_stopwords,
                         stem = F)

out <- prepDocuments(df_prep$documents,
                     df_prep$vocab,
                     df_prep$meta,
                     verbose = F,
                     lower.thresh = 5,
                     upper.thresh = floor(nrow(df_clean_final) * .9))

docs <- out$documents
vocab <- out$vocab 
meta <- out$meta
```

Determine what a reasonable number of topics is.

```{r eval = F}
plan(multiprocess)

k <- c(5, 10, 20, 40, 60, 80, 100, 120, 150, 200)
many_models <- tibble(K = k) %>%
  mutate(topic_model = future_map(K, ~stm(documents = out$documents, 
                                          vocab = out$vocab, data = out$meta, K = .,
                                          prevalence = ~ field + s(months_since_first_pub),
                                          verbose = FALSE)))
  
```

```{r}
heldout <- make.heldout(documents = out$documents, vocab = out$vocab)

k_result <- many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, out$documents),
         eval_heldout = map(topic_model, eval.heldout, heldout$missing),
         residual = map(topic_model, checkResiduals, out$documents),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

k_result
```

```{r}
k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 110")

k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  unnest() %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Exclusivity and semantic coherence")
```

```{r eval = F}
# creating topic model with 110 topics and three independent variables for topic prevalence.
number_of_topics <- 110

df_stm <- stm(documents = out$documents,
              vocab = out$vocab,
              K = number_of_topics,
              prevalence = ~ field + s(months_since_first_pub),
              data = out$meta,
              init.type = "Spectral",
              verbose = F)

df_stm_lda <- stm(documents = out$documents,
              vocab = out$vocab,
              K = number_of_topics,
              prevalence = ~ field + s(months_since_first_pub),
              data = out$meta,
              init.type = "LDA",
              seed = 1234,
              verbose = F)
```

```{r Visualization Spectral}
df_stm_td <- tidy(df_stm)

top_terms <- df_stm_td %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>%
  unnest(cols = c(terms))

td_gamma <- tidy(df_stm, matrix = "gamma",
                 document_names = rownames(df_stm))

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

gamma_terms %>%
  top_n(20, gamma) %>%
  ggplot(aes(topic, gamma, label = terms)) +
  geom_col(aes(color = "black"), alpha = 0.8, show.legend = FALSE, color = "white") +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.09),
                     labels = percent_format()) +
  labs(x = NULL, y = expression(gamma),
       title = "Top 20 topics by prevalence",
       subtitle = "With the top words that contribute to each topic") 

gamma_terms %>%
  kable() %>%
  kable_classic("striped", full_width = F)
```

```{r Visualization LDA}
df_stm_td <- tidy(df_stm_lda)

top_terms <- df_stm_td %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>%
  unnest(cols = c(terms))

td_gamma <- tidy(df_stm_lda, matrix = "gamma",
                 document_names = rownames(df_stm_lda))

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

gamma_terms %>%
  top_n(20, gamma) %>%
  ggplot(aes(topic, gamma, label = terms)) +
  geom_col(aes(color = "black"), alpha = 0.8, show.legend = FALSE, color = "white") +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.09),
                     labels = percent_format()) +
  labs(x = NULL, y = expression(gamma),
       title = "Top 20 topics by prevalence",
       subtitle = "With the top words that contribute to each topic") 

gamma_terms %>%
  kable() %>%
  kable_classic("striped", full_width = F)


```

Topic Correlations.

```{r}
topic_cor <- topicCorr(df_stm, cutoff = .03)
net <- network::network(topic_cor$posadj, directed = F)
ggnet2(net, label = T)
```


LDA Vis.

```{r}
toLDAvis(df_stm, docs)
```

```{r}
toLDAvis(df_stm_lda, docs)
```


```{r Covariates}
number_of_topics <- 110
out$meta$field = as.factor(out$meta$field)
prep <- estimateEffect(1:number_of_topics ~ field + s(months_since_first_pub),
                       df_stm,
                       meta = out$meta,
                       uncertainty = "Global")

result_reg <- tidy(prep) %>%
  mutate(p.value = round(p.value, digits = 3),
         term = str_remove(term, "field"))
  
result_reg %>%
  mutate(p.value = cell_spec(p.value,
                         "html",
                         color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
  kable(format = "html", escape = F) %>%
  kable_classic("striped", full_width = F)
```


```{r eval = F}
result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate) %>%
  mutate(p.value = cell_spec(p.value,
                         "html",
                         color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
  kable(format = "html", escape = F) %>%
  kable_classic("striped", full_width = F)

topic_best_term <- result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate)
```

```{r}
net <- topicCorr(df_stm, cutoff = 0.05)
net <- net$posadj
net <- network::network(net, directed = F)
net %v% "term" = as.character(topic_best_term$term)
net %v% "topic" = as.character(topic_best_term$topic)

y = RColorBrewer::brewer.pal(9, "Set3")
names(y) = levels(as.factor(topic_best_term$term))

ggnet2(net, color = "term", palette = y, alpha = 0.75,
       size = 4, edge.alpha = 0.5,node.label = "topic",
       node.size = 5) +
  labs(title = "Topic Correlations",
       subtitle = "Nodes indicate topics, edges indicate correlation between topics") 
```

```{r Temporality}
plot(prep, "months_since_first_pub",
     method = "continuous", topics = c(31, 43),
     model = z, printlegend = F,
     xaxt = "n", xlab = "Time (1991-2020)")
monthseq <- seq(from = as.Date("1990-01-01"),
                to = as.Date("2021-12-31"), by = "month")
monthnames <- months(monthseq)
axis(1,at = as.numeric(monthseq) - min(as.numeric(monthseq)))
```

```{r}
labelTopics(df_stm, topics = c(31, 43))
```




</body>