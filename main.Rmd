---
title: "Structural Topic Model - Government-Business Relationship in Academic Literature"
author: "Václav Ocelík"
date: "`r Sys.Date()`"
output: html_document
---

<style type="text/css">
.main-container {
  max-width: 1200px;
  margin-left: auto;
  margin-right: auto;
}
h1.title {
  font-size: 38px;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  text-align: center;
</style>

```{r setup, include=FALSE, message = F, echo = F}
knitr::opts_chunk$set(echo = T, message = F, error = F, dpi = 400, fig.width = 8,
                      fig.height = 6, warning = F)
```

<body>
__The__ following is a structural topic model analysis of abstract from academic articles on the relationship between politics and business published between 1990 and 2020. The objective of the analysis is to discover novel avenues for research in the field of corporate political activity. Corporate political activity has been researched by political scientists, sociologists, economists, and business scholars alike. Yet these scholars remain fragmented by their disciplinary loyalties. The topic model will reveal the underlying themes in large collections of articles, which will allows us to explore how the different fields have dealt with corporate political activity, as well as speculating on structural holes that divide the different fields. This data-mining approach to literature reviews leverages recent advancements in the field of unsupervised machine learning to analyze an enormous number of article abstracts. 

The articles were acquired through _Thomson Reuters Web of Knowledge_. The following keywords search was used:  

<strong>AB = ( (business\*,  OR firm\*  OR corporat\*  OR compan\*  OR enterpris\*  OR entrepreneur\*)  AND (government\*  OR bureaucra\*  OR minist\*  OR legislat\*  OR judicia\*  OR executiv\* ) . </strong>  

This resulted in <strong>75.931 </strong> hits. I have thus far extracted the 30.000 most relevant (sorted by Web of Knowledge) Not all of these hits are relevant and/or suitable to the analysis, however. The dataset needs to be cleaned before it can be used in the structural topic model. 

The following libraries are required for the analysis.

```{r Libraries}
#setwd("/Users/vavaocel/Document/projects/CPA_stm")
rm(list=ls())

library(furrr)
library(GGally)
library(ggthemes)
library(gridExtra)
library(knitr)
library(kableExtra)
library(lubridate)
library(network)
library(readxl)
library(tidylo)
library(tidytext)
library(tidyverse)
library(scales)
library(stm)
library(tm)

load("finalrun.RData")

theme_set(theme_classic())
```

We import the data iteratively, as it is divided over 50+ excel files. The folder containing the data should not include excel files other than the Web of Science data.

```{r Import and Combine Data, echo = T}
# Import data ####
df <- do.call(rbind, lapply(Sys.glob("data/data_sept_2020/*.xls"), read_excel)) # make sure no other .xls files in target.
```

```{r eval = F, echo = F}
# A list of all variables.
relevant_cols <- df %>%
  janitor::clean_names() %>%
  select(abstract, publication_year, article_title, times_cited_all_databases,
         source_title, document_type) %>%
  names() 

relevant_row_nums <- df %>%
  janitor::clean_names() %>% 
  names() %>%
  as_tibble_col(column_name = "Variable") %>%
  filter(Variable != "x68") %>%
  mutate(row_num = row_number()) %>% 
  filter(Variable %in% relevant_cols) %>%
  pull(row_num)

df %>%
  janitor::clean_names() %>%
  names() %>%
  as_tibble_col(column_name = "Variable") %>%
  filter(Variable != "x68") %>%
  kbl(caption = "All variables from the data set provided by WOS.") %>%
  kable_classic("striped", full_width = F) %>%
  row_spec(relevant_row_nums, bold = T) %>%
  footnote(general = "Bold names are suggestions for use in the structural topic model.",
           number = "The variable 'academic field' needs to be created.")

df %>%
  janitor::clean_names() %>%
  names() %>%
  as_tibble_col(column_name = "Variable") %>%
  filter(Variable != "x68") %>%
  htmlTable::htmlTable(n.cgroup = c(2,2),
                       rnames = F)


rm(relevant_cols, relevant_row_nums)
```

```{r}
# keep only the necessary columns
df_clean <- df %>%
  janitor::clean_names() %>%
  select(authors, `article_title`, `source_title`, abstract,
         `publication_year`, `times_cited_all_databases`,
         `author_keywords`, `keywords_plus`, publication_date,
         issn) %>%
  add_count(article_title) %>%
  filter(n == 1) %>% # remove duplicates
  select(-n)

# remove rows with missing years and abstracts.
df_clean <- df_clean %>%
  filter(!is.na(`publication_year`),
         !is.na(abstract)) %>%
  mutate(ID = row_number(),
         abstract = str_remove(abstract, "[(][C-c][)].*$"),
         abstract_Length = str_length(abstract))
#rm(df)
```

A small random sample from the data.

```{r}
df_clean %>%
  select(authors, `article_title`, `publication_year`, `times_cited_all_databases`,
         abstract) %>%
  drop_na() %>%
  sample_n(5) %>%
  kbl(caption = "A snapshot of the data") %>%
  kable_classic("striped", full_width = FALSE) %>%
  footnote(general = "Rows were randomly selected.")
```

```{r eval = F, echo = F} 
# Creating boxplots and histogram of citation count.
cite_boxplot <- df_clean %>%
  ggplot(aes(times_cited_all_databases)) +
  geom_boxplot(outlier.alpha = .7) +
  coord_flip() +
  labs(x = NULL,
       y = "citations")

cite_boxplot_log <- df_clean %>%
  ggplot(aes(log(times_cited_all_databases))) +
  geom_boxplot(outlier.alpha = .7) +
  coord_flip() +
  labs(x = NULL,
       y = "log(citations)")

cite_hist <- df_clean %>%
  ggplot(aes(times_cited_all_databases)) +
  geom_histogram(color = "white") +
  labs(y = NULL,
       x = "citations")

cite_hist_log <- df_clean %>%
  ggplot(aes(log(times_cited_all_databases))) +
  geom_histogram(color = "white") +
  labs(y = NULL,
       x = "log(citations)")

grid.arrange(cite_boxplot, cite_boxplot_log,
             ncol = 2,
             top = "Boxplot comparison of citations and log(citations).",
             left = "times cited")

grid.arrange(cite_hist, cite_hist_log,
             ncol = 2,
             top = "Histogram comparison of citations and log(citations).",
             left = "Count",
             bottom = "Citation count")
rm(cite_boxplot, cite_boxplot_log, cite_hist, cite_hist_log)
# As we can see, taking the natural logarithm mitigates the skewness somewhat, but long right-tails remain. 
```

We now transform some variables and add others.
```{r Additional Variables and Join}
# creating some additional variables
# df_clean_y <- df_clean %>%
#   filter(!is.na(`publication_year`),
#          !is.na(abstract),
#          !is.na(publication_date)) %>%
#   mutate(ID = row_number(),
#          abstract_Length = str_length(abstract),
#          publication_date = case_when(publication_date == "JAN" ~ 1,
#                                       publication_date == "FEB" ~ 2,
#                                       publication_date == "MAR" ~ 3,
#                                       publication_date == "APR" ~ 4,
#                                       publication_date == "MAY" ~ 5,
#                                       publication_date == "JUN" ~ 6,
#                                       publication_date == "JUL" ~ 7,
#                                       publication_date == "AUG" ~ 8,
#                                       publication_date == "SEP" ~ 9,
#                                       publication_date == "OCT" ~ 10,
#                                       publication_date == "NOV" ~ 11,
#                                       publication_date == "DEC" ~ 12)) %>%
#   filter(!is.na(publication_date)) %>%
#   unite("date", publication_year, publication_date, sep = "-") %>%
#   mutate(date = paste(date, "01", sep = "-"), date = as.Date(date),
#          months_since_first_pub = interval(min(date), date),
#          months_since_first_pub = months_since_first_pub %/% months(1) + 1) 
```

Next, I import the journal data from the [Web of Science Master Journal List](https://mjl.clarivate.com/collection-list-downloads). I use the Social Sciences Citation Index (SSCI). Joining this dataframe on *df_clean* allows us to use the main field as metadata in the structural topic model. I will keep the 7 most common fields in the dataset.   

```{r}
df_wos <- read_csv("data/wos-core_SSCI.csv") %>%
  select(source_title = `Journal title`,`Web of Science Categories`, issn = ISSN) 

df_clean <- df_clean %>%
  left_join(distinct(df_wos)) 

df_clean <- df_clean %>%
  filter(!is.na(`Web of Science Categories`)) %>%
  mutate(field = str_remove(`Web of Science Categories`, ' [.]*[ |.].*'),
         field = str_remove(field, '[,].*'))

rm(df_wos)

df_clean %>%
  count(field, sort = T, name = "count") %>%
  top_n(10) %>%
  kbl(caption = "The 7 most common fields to research the relationship between business and politics.") %>%
  kable_classic("striped")

fields <- df_clean %>%
  mutate(field = ifelse(field == "International Relations" | field == "Public Administration", "Political Science", field),
         field = ifelse(field == "Sociology" | field == "Anthropology" | field == "Cultural Studies", "Social Sciences", field)) %>%
  count(field, sort = T) %>%
  top_n(7) %>%
  pull(field)

df_clean <- distinct(df_clean)
```

## Some descriptive exploration

What are the most common journals?

```{r}
df_clean %>%
  count(source_title, sort = T, name = "number of publications") %>%
  head(20) %>%
  mutate(source_title = str_to_lower(source_title)) %>%
  kbl(caption = "The most common journals to publish research on the relationship between business and politics.") %>%
  kable_classic("striped")
```

What are the most common keywords?

```{r}
df_clean %>%
  filter(!is.na(`keywords_plus`)) %>%
  separate(`keywords_plus`, sep = ";", into = "keywords") %>%
  mutate(keywords = tolower(keywords),
         keywords = str_replace_all(string = keywords, pattern = "-", replacement = " "),
         keywords = str_replace_all(string  = keywords, pattern = "([.*])", replacement = "")) %>%
  count(keywords, sort = T, name = "count") %>%
  head(20) %>%
  kbl(caption = "The most popular keywords") %>%
  kable_classic("striped", full_width = FALSE)
```

Exploring the most popular keywords per scientific field.

```{r}
df_clean %>%
  filter(!is.na(`keywords_plus`)) %>%
  separate(`keywords_plus`, sep = ";", into = "keywords") %>%
  mutate(keywords = tolower(keywords),
         keywords = str_replace_all(keywords, "-", " ")) %>%
  group_by(field, keywords) %>%
  summarise(count = n()) %>%
  filter(count == max(count),
         count > 1) %>%
  arrange(-count) %>%
  head(25) %>%
  kbl(caption = "The most popular keywords by field.") %>%
  kable_classic("striped", full_width = FALSE) %>%
  footnote(general = "Keywords provided by Web of Science")
  
df_clean %>%
  filter(!is.na(`author_keywords`)) %>%
  separate(`author_keywords`, sep = ";", into = "keywords") %>%
  mutate(keywords = tolower(keywords),
         keywords = str_replace_all(keywords, "-", " ")) %>%
  group_by(field, keywords) %>%
  summarise(count = n()) %>%
  filter(count == max(count),
         count > 1) %>%
  arrange(-count) %>%
  head(25) %>%
  kbl(caption = "The most popular keywords by field.") %>%
  kable_classic("striped", full_width = FALSE) %>%
  footnote(general = "Keywords provided by authors.")

```

We now recode some entries in the `field` variable. 

```{r}
tbl <-
    list.files(pattern = "*.csv") %>% 
    map_df(~read_csv(.)) %>%
    janitor::clean_names() %>%
    select(x2:x8) 

colnames(tbl) <- as.character(as.vector(tbl[1,]))

df_clean <- df_clean %>%
    mutate(ISSN = issn) %>%
    left_join(tbl) %>%
    distinct() %>%
    janitor::clean_names()

rm(tbl)

df_clean_final <- df_clean %>%
  mutate(field = ifelse(field =="International Relations" | field == "Public Administration", "Political Science", field),
         field = ifelse(field == "Communication" | field == "Sociology" | field == "Anthropology" | field == "Cultural Studies", "Social Sciences", field)) %>%
  filter(field %in% fields) %>%
  filter(!is.na(full_journal_title)) %>%
  mutate(total_cites = str_remove(total_cites,"[,]"),
         total_cites = as.numeric(total_cites),
         journal_impact_factor = as.double(journal_impact_factor),
         impact_factor_without_journal_self_cites = as.double(impact_factor_without_journal_self_cites),
         x5_year_impact_factor = as.double(x5_year_impact_factor),
         eigenfactor_score = as.double(eigenfactor_score)) 


df_clean_final <- df_clean_final %>%
  mutate(length = str_length(abstract)) %>%
  filter(length < 3000, 
         length > 500) %>%
  select(-length) # drops 305 documents
```

The data set has now been reduced to `r nrow(df_clean_final)` articles. 

```{r eval = F, echo = F}
### world-level analysis

# I will now first do some exploratory word-level analysis. This includes calculating the _tf_idf_ and the weighted log-odds of the terms. Stop words are obviously removed prior to the calculations.

# extract the words out of the abstracts.
df_words <- df_clean_final %>%
  unnest_tokens(word, abstract) %>%
  anti_join(stop_words) %>%
  filter(!str_detect(word, "\\d"),
         !str_detect(word, "\\."),
         str_length(word) > 2,
         !word %in% c("xxx"))

custom_stopwords <- df_words %>%
  count(word, sort = T) %>%
  top_n(20) %>%
  pull(word)

# calculate tf_idf 
df_words_tf_idf <- df_words %>%
  count(word, field, sort = T) %>%
  bind_tf_idf(word, field, n) %>%
  arrange(-tf_idf) %>%
  group_by(field) %>%
  top_n(10) %>%
  ungroup()

# plot tf_idf
df_words_tf_idf %>%
  mutate(word = reorder_within(word, tf_idf, field)) %>%
  ggplot(aes(word, tf_idf, fill = field)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ field, scales = "free") +
  scale_x_reordered() +
  coord_flip() +
  labs(title = "TF-IDF words per Academic Field.")

# calculate log_odds
df_words_log_odds <- df_words %>%
  count(field, word) %>%
  bind_log_odds(field, word, n) %>%
  arrange(-log_odds_weighted)

# plot log_odds
df_words_log_odds %>%
  group_by(field) %>%
  slice_max(log_odds_weighted, n = 10) %>%
  mutate(word = reorder(word, log_odds_weighted)) %>%
  ungroup() %>%
  ggplot(aes(log_odds_weighted, word, fill = field)) +
  geom_col(alpha = 0.8, show.legend = F) +
  facet_wrap(~field, scales = "free") +
  scale_y_reordered() +
  labs(y = NULL,
       title = "Most propobable terms per Academic Field.") 

rm(df_words, df_words_log_odds, df_words_tf_idf)
```

```{r Additional Graphs, eval = F,echo=F}
# Before proceeding with the topic model, I include some additional graphs
# to gain a deeper understanding of the structure of the dataset.
# plot variance in citation count per field and per type. 
df_clean_final %>%
  ggplot(aes(reorder(field, times_cited_all_databases, median), times_cited_all_databases)) +
  geom_boxplot() +
  labs(y = NULL,
       x = NULL,
       title = "Boxplot of Average Citation Count per Field",
       subtitle = "With Type as subcategory."
       )

# plot histogram of publications per type. 
df_clean_final %>%
  ggplot(aes(`months_since_first_pub`)) +
  geom_histogram(color = "white", bins = 30) +
  scale_fill_economist() +
  labs(y = NULL,
       x = "Years since first publication.",
       title = "Histogram of publications on CPA.",
       subtitle = "From 1990 to Present")

# plot probability density of publications per type. 
df_clean_final %>%
  ggplot(aes(months_since_first_pub, ..density..)) +
  geom_freqpoly() +
  scale_color_wsj() +
  labs(y = NULL,
       x = NULL,
       title = "Probability density of publications on CPA.") 
```

## Structural Topic Model
 
Now, I proceed with the structural topic model. The following steps are based on the _stm_ vignette by [Roberts et al. (2014).](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf). I also apply some code written by [Julia Silge](https://juliasilge.com/blog/evaluating-stm/) which allows for helpful plots not available in the stm package.

### Preprocessing

```{r}
df_prep <- textProcessor(df_clean_final$abstract,
                         metadata = df_clean_final,
                         verbose = F,
                         onlycharacter = T,
                         stem = F,
                         removepunctuation = T,
                         removenumbers = T)
```

```{r}
out <- prepDocuments(df_prep$documents,
                     df_prep$vocab,
                     df_prep$meta,
                     verbose = F,
                     lower.thresh = 15,
                     upper.thresh = floor(nrow(df_clean_final) * .2)) # removes 12 most common words

docs <- out$documents
vocab <- out$vocab 
meta <- out$meta
```

Check which words were removed. We want to remove words that are very frequent and very infrequent. This improves topic model estimation and interpretation.
```{r}
df_clean_final %>%
  unnest_tokens(word, abstract) %>%
  anti_join(stop_words) %>%
  count(word, sort = T) %>%
  rename(value = word, count = n) %>%
  left_join(as_tibble(out$words.removed) %>% add_count(value)) %>% filter (!is.na(n)) %>%
  select(-n) %>%
  head(20) %>%
  kable(caption = "Top 20 words removed due to (in)frequency") %>%
  kable_classic("striped", full_width = F)
```

### Setting K

Determine what a reasonable number of topics is. There is <strong>not</strong> a single optimal number of topics for any set of corpora, but we can apply certain estimation methods to find a range of reasonable topic models. We explore the full range and select a specific topic model on the basis of criteria we elaborate on in the article.

```{r}
k <- c(40, 70, 100, 150, 200)
```

We have already imported the result of a previous run to avoid unnecessary computation. However, the following code was used to create the models:  

```{r eval = F, echo = T}
# Spectral algorithm takes +- 1.5 hours
plan(multicore)

many_models <- tibble(K = k) %>%
  mutate(topic_model = future_map(K, ~stm(documents = out$documents, vocab = out$vocab, 
                                          data = out$meta, K = . ,
                                          prevalence = ~ field + s(publication_year) + s(times_cited_all_databases) + s(impact_factor_without_journal_self_cites), verbose = FALSE)))

# The LDA algorithm is rather slow so the following code can take 9+ hours to run.
# The LDA algorithm requires many more iterations.
# plan(multicore)
# 
# many_models_LDA <- tibble(K = k) %>%
#   mutate(topic_model = future_map(K, ~stm(documents = out$documents, 
#                                           vocab = out$vocab, data = out$meta, K = .,
#                                           prevalence = ~ field + s(publication_year) + s(times_cited_all_databases) + s(impact_factor_without_journal_self_cites),
#                                           verbose = FALSE,
#                                           init.type = "LDA",
#                                          seed = 1234)))
```

Inspect results. Notice how the Spectral algorithm requires far less (basically a magnitude) iterations. 

```{r}
heldout <- make.heldout(documents = out$documents, vocab = out$vocab)
```

```{r eval = F}
k_result <- many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, out$documents),
         eval_heldout = map(topic_model, eval.heldout, heldout$missing),
         residual = map(topic_model, checkResiduals, out$documents),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

k_result

# k_result_LDA <- many_models_LDA %>%
#   mutate(exclusivity = map(topic_model, exclusivity),
#          semantic_coherence = map(topic_model, semanticCoherence, out$documents),
#          eval_heldout = map(topic_model, eval.heldout, heldout$missing),
#          residual = map(topic_model, checkResiduals, out$documents),
#          bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
#          lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
#          lbound = bound + lfact,
#          iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))
# 
#k_result_LDA
```

```{r}
k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 60-100")

k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  unnest() %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Exclusivity and semantic coherence")

# k_result_LDA %>%
#   transmute(K,
#             `Lower bound` = lbound,
#             Residuals = map_dbl(residual, "dispersion"),
#             `Semantic coherence` = map_dbl(semantic_coherence, mean),
#             `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
#   gather(Metric, Value, -K) %>%
#   ggplot(aes(K, Value, color = Metric)) +
#   geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
#   facet_wrap(~Metric, scales = "free_y") +
#   labs(x = "K (number of topics)",
#        y = NULL,
#        title = "Model diagnostics by number of topics",
#        subtitle = "These diagnostics indicate that a good number of topics would be around 60-100")
# 
# k_result_LDA %>%
#   select(K, exclusivity, semantic_coherence) %>%
#   unnest() %>%
#   mutate(K = as.factor(K)) %>%
#   ggplot(aes(semantic_coherence, exclusivity, color = K)) +
#   geom_point(size = 2, alpha = 0.7) +
#   labs(x = "Semantic coherence",
#        y = "Exclusivity",
#        title = "Exclusivity and semantic coherence")

#rm(k_result, k_result_LDA)
```

## Evaluating the topic models

Now, we proceed to evaluate the topic models. For each run in `k` from 40 onward, we plot the 30% most prevalane topic and their top terms. In addition, we run regression analysis on the topic prevalence to determine to what field the topic belongs. We then create tables with each topic, its gamma, its top terms based on four different measurements: highest prob, FEX, Score, and Lift. For details on these measurements, see the STM vignette. Finally, we search the document that best fits a topic and include this in the table as well. Finally, we create a network graph where nodes denote topics and edges denote correlation < 0.05. We also use the `LDAvis` package to view our intertopic distance map.

```{r}
i <- 150
```

```{r out.width = "600px", echo=F}
knitr::include_graphics("images/Screenshot 2020-09-20 at 22.23.10.png")
```


```{r}
df_stm <- k_result %>% 
  filter(K == i) %>% 
  pull(topic_model) %>% 
  .[[1]]

df_stm_td <- tidy(df_stm)

top_terms <- df_stm_td %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>%
  unnest(cols = c(terms))

td_gamma <- tidy(df_stm, matrix = "gamma",
                 document_names = rownames(df_stm))

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

gamma_terms %>%
  top_n(floor(nrow(gamma_terms) * .3), gamma) %>%
  ggplot(aes(topic, gamma, label = terms)) +
  geom_col(aes(color = "black"), alpha = 0.8, show.legend = FALSE, color = "white") +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, gamma_terms %>% pull(gamma) %>% max() * 5),
                     labels = percent_format()) +
  labs(x = NULL, y = expression(gamma),
       title = "Top 20 topics by prevalence",
       subtitle = "With the top words that contribute to each topic") 

# gamma_terms %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

# LDA visualization
toLDAvis(df_stm, docs)

# covariates
out$meta$field = as.factor(out$meta$field)
prep <- estimateEffect(1:i ~ field + s(publication_year) + s(times_cited_all_databases) + s(impact_factor_without_journal_self_cites),
                       df_stm,
                       meta = out$meta,
                       uncertainty = "Global")

result_reg <- tidy(prep) %>%
  mutate(p.value = round(p.value, digits = 3),
         term = str_remove(term, "field"))
  
# result_reg %>%
#   mutate(p.value = cell_spec(p.value, "html", color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
#   kable(format = "html", escape = F) %>%
#   kable_classic("striped", full_width = F)

topic_field_matched <- result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate) 

# topic_field_matched %>%
#   mutate(p.value = cell_spec(p.value, "html", color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
#   kable(format = "html", escape = F) %>%
#   kable_classic("striped", full_width = F)

topic_best_term <- result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate)

topic_field_gamma <- gamma_terms %>%
  mutate(topic = as.factor(trimws(str_remove(topic, "Topic ")))) %>%
  left_join(topic_field_matched %>% select(topic, term) %>% mutate(topic = as.factor(topic)) %>% rename(field = term)) %>%
  select(topic, field, gamma, terms) 

# topic_field_gamma %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

a <- labelTopics(df_stm, topics = c(1:i))

topic_full_description <- tibble(Topic = a$topicnums,
       frex = as_tibble(a$frex) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       prob = as_tibble(a$prob) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       lift = as_tibble(a$lift) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       score = as_tibble(a$score) %>% unite(frex, V1:V7, sep = ", ") %>% pull())

topic_full_table <- topic_field_gamma %>%
  left_join(topic_full_description %>% janitor::clean_names() %>% mutate(topic = as.factor(topic))) %>%
  select(-terms)

# topic_full_table %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

short_docs <- meta %>%
    as_tibble() %>%
    select(abstract) %>%
    mutate(abstract = str_sub(abstract, 1, 800))

topic_thoughts <- findThoughts(df_stm, topic = c(1:i), texts = short_docs$abstract, n = 1)

topic_top_docs <- as_tibble(topic_thoughts$index) %>%
  janitor::clean_names() %>%
  pivot_longer(everything()) %>%
  left_join(short_docs %>% mutate(value = row_number())) %>%
  rename(topic = name, doc_index = value, short_abstract = abstract) %>%
  mutate(topic = str_remove(topic, "topic_")) 

table_final <- topic_full_table %>%
  left_join(topic_top_docs %>% mutate(topic = as.factor(topic))) %>%
  select(-doc_index) %>%
  kable() %>%
  kable_styling()
table_final

# network
net <- topicCorr(df_stm, cutoff = 0.05)
net <- net$posadj
net <- network::network(net, directed = F)
net %v% "term" = as.character(topic_best_term$term)
net %v% "topic" = as.character(topic_best_term$topic)

y = RColorBrewer::brewer.pal(9, "Set3")
names(y) = levels(as.factor(topic_best_term$term))

ggnet2(net, color = "term", palette = y, alpha = 0.75,
       size = 4, edge.alpha = 0.5,node.label = "topic",
       node.size = 5) +
  labs(title = "Topic Correlations",
       subtitle = "Nodes indicate topics, edges indicate correlation between topics")

#rm(net, topic_best_term)

# plot(prep, "months_since_first_pub",
#      method = "continuous", topics = c(,),
#      model = z, printlegend = F,
#      xaxt = "n", xlab = "Time (1991-2020)")
# monthseq <- seq(from = as.Date("1990-01-01"),
#                 to = as.Date("2021-12-31"), by = "month")
# monthnames <- months(monthseq)
# axis(1,at = as.numeric(monthseq) - min(as.numeric(monthseq)))
# 
```


```{r}
i <- 60
```


```{r out.width = "600px", echo=F}
knitr::include_graphics("images/Screenshot 2020-09-20 at 22.23.40.png")
```


```{r}
df_stm <- k_result %>% 
  filter(K == i) %>% 
  pull(topic_model) %>% 
  .[[1]]

df_stm_td <- tidy(df_stm)

top_terms <- df_stm_td %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>%
  unnest(cols = c(terms))

td_gamma <- tidy(df_stm, matrix = "gamma",
                 document_names = rownames(df_stm))

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

gamma_terms %>%
  top_n(floor(nrow(gamma_terms) * .3), gamma) %>%
  ggplot(aes(topic, gamma, label = terms)) +
  geom_col(aes(color = "black"), alpha = 0.8, show.legend = FALSE, color = "white") +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, gamma_terms %>% pull(gamma) %>% max() * 5),
                     labels = percent_format()) +
  labs(x = NULL, y = expression(gamma),
       title = "Top 20 topics by prevalence",
       subtitle = "With the top words that contribute to each topic") 

# gamma_terms %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

# LDA visualization
toLDAvis(df_stm, docs)

# covariates
out$meta$field = as.factor(out$meta$field)
prep <- estimateEffect(1:i ~ field + s(publication_year) + s(times_cited_all_databases) + s(impact_factor_without_journal_self_cites),
                       df_stm,
                       meta = out$meta,
                       uncertainty = "Global")

result_reg <- tidy(prep) %>%
  mutate(p.value = round(p.value, digits = 3),
         term = str_remove(term, "field"))
  
# result_reg %>%
#   mutate(p.value = cell_spec(p.value, "html", color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
#   kable(format = "html", escape = F) %>%
#   kable_classic("striped", full_width = F)

topic_field_matched <- result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate) 

# topic_field_matched %>%
#   mutate(p.value = cell_spec(p.value, "html", color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
#   kable(format = "html", escape = F) %>%
#   kable_classic("striped", full_width = F)

topic_best_term <- result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate)

topic_field_gamma <- gamma_terms %>%
  mutate(topic = as.factor(trimws(str_remove(topic, "Topic ")))) %>%
  left_join(topic_field_matched %>% select(topic, term) %>% mutate(topic = as.factor(topic)) %>% rename(field = term)) %>%
  select(topic, field, gamma, terms) 

# topic_field_gamma %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

a <- labelTopics(df_stm, topics = c(1:i))

topic_full_description <- tibble(Topic = a$topicnums,
       frex = as_tibble(a$frex) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       prob = as_tibble(a$prob) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       lift = as_tibble(a$lift) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       score = as_tibble(a$score) %>% unite(frex, V1:V7, sep = ", ") %>% pull())

topic_full_table <- topic_field_gamma %>%
  left_join(topic_full_description %>% janitor::clean_names() %>% mutate(topic = as.factor(topic))) %>%
  select(-terms)

topic_full_table %>%
  kable() %>%
  kable_classic("striped", full_width = F)

short_docs <- meta %>%
    as_tibble() %>%
    select(abstract) %>%
    mutate(abstract = str_sub(abstract, 1, 800))

topic_thoughts <- findThoughts(df_stm, topic = c(1:i), texts = short_docs$abstract, n = 1)

topic_top_docs <- as_tibble(topic_thoughts$index) %>%
  janitor::clean_names() %>%
  pivot_longer(everything()) %>%
  left_join(short_docs %>% mutate(value = row_number())) %>%
  rename(topic = name, doc_index = value, short_abstract = abstract) %>%
  mutate(topic = str_remove(topic, "topic_")) 

table_final <- topic_full_table %>%
  left_join(topic_top_docs) %>%
  select(-doc_index) %>%
  kable() %>%
  kable_classic("striped", full_width = F)
table_final

# network
net <- topicCorr(df_stm, cutoff = 0.05)
net <- net$posadj
net <- network::network(net, directed = F)
net %v% "term" = as.character(topic_best_term$term)
net %v% "topic" = as.character(topic_best_term$topic)

y = RColorBrewer::brewer.pal(9, "Set3")
names(y) = levels(as.factor(topic_best_term$term))

ggnet2(net, color = "term", palette = y, alpha = 0.75,
       size = 4, edge.alpha = 0.5,node.label = "topic",
       node.size = 5) +
  labs(title = "Topic Correlations",
       subtitle = "Nodes indicate topics, edges indicate correlation between topics")

#rm(net, topic_best_term)

# plot(prep, "months_since_first_pub",
#      method = "continuous", topics = c(,),
#      model = z, printlegend = F,
#      xaxt = "n", xlab = "Time (1991-2020)")
# monthseq <- seq(from = as.Date("1990-01-01"),
#                 to = as.Date("2021-12-31"), by = "month")
# monthnames <- months(monthseq)
# axis(1,at = as.numeric(monthseq) - min(as.numeric(monthseq)))
# 
```

```{r}
i <- 80
```



```{r out.width = "600px", echo=F}
knitr::include_graphics("images/Screenshot 2020-09-20 at 22.23.45.png")
```


```{r}
df_stm <- k_result %>% 
  filter(K == i) %>% 
  pull(topic_model) %>% 
  .[[1]]

df_stm_td <- tidy(df_stm)

top_terms <- df_stm_td %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>%
  unnest(cols = c(terms))

td_gamma <- tidy(df_stm, matrix = "gamma",
                 document_names = rownames(df_stm))

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

gamma_terms %>%
  top_n(floor(nrow(gamma_terms) * .3), gamma) %>%
  ggplot(aes(topic, gamma, label = terms)) +
  geom_col(aes(color = "black"), alpha = 0.8, show.legend = FALSE, color = "white") +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, gamma_terms %>% pull(gamma) %>% max() * 5),
                     labels = percent_format()) +
  labs(x = NULL, y = expression(gamma),
       title = "Top 20 topics by prevalence",
       subtitle = "With the top words that contribute to each topic") 

# gamma_terms %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

# LDA visualization
toLDAvis(df_stm, docs)

# covariates
out$meta$field = as.factor(out$meta$field)
prep <- estimateEffect(1:i ~ prevalence = ~ field + s(publication_year) + s(times_cited_all_databases) + s(impact_factor_without_journal_self_cites),
                       df_stm,
                       meta = out$meta,
                       uncertainty = "Global")

result_reg <- tidy(prep) %>%
  mutate(p.value = round(p.value, digits = 3),
         term = str_remove(term, "field"))
  
# result_reg %>%
#   mutate(p.value = cell_spec(p.value, "html", color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
#   kable(format = "html", escape = F) %>%
#   kable_classic("striped", full_width = F)

topic_field_matched <- result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate) 

# topic_field_matched %>%
#   mutate(p.value = cell_spec(p.value, "html", color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
#   kable(format = "html", escape = F) %>%
#   kable_classic("striped", full_width = F)

topic_best_term <- result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate)

topic_field_gamma <- gamma_terms %>%
  mutate(topic = as.factor(trimws(str_remove(topic, "Topic ")))) %>%
  left_join(topic_field_matched %>% select(topic, term) %>% mutate(topic = as.factor(topic)) %>% rename(field = term)) %>%
  select(topic, field, gamma, terms) 

# topic_field_gamma %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

a <- labelTopics(df_stm, topics = c(1:i))

topic_full_description <- tibble(Topic = a$topicnums,
       frex = as_tibble(a$frex) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       prob = as_tibble(a$prob) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       lift = as_tibble(a$lift) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       score = as_tibble(a$score) %>% unite(frex, V1:V7, sep = ", ") %>% pull())

topic_full_table <- topic_field_gamma %>%
  left_join(topic_full_description %>% janitor::clean_names() %>% mutate(topic = as.factor(topic))) %>%
  select(-terms)

# topic_full_table %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

short_docs <- meta %>%
    as_tibble() %>%
    select(abstract) %>%
    mutate(abstract = str_sub(abstract, 1, 800))

topic_thoughts <- findThoughts(df_stm, topic = c(1:i), texts = short_docs$abstract, n = 1)

topic_top_docs <- as_tibble(topic_thoughts$index) %>%
  janitor::clean_names() %>%
  pivot_longer(everything()) %>%
  left_join(short_docs %>% mutate(value = row_number())) %>%
  rename(topic = name, doc_index = value, short_abstract = abstract) %>%
  mutate(topic = str_remove(topic, "topic_")) 

table_final <- topic_full_table %>%
  left_join(topic_top_docs) %>%
  select(-doc_index) %>%
  kable() %>%
  kable_classic("striped", full_width = F)
table_final

# network
net <- topicCorr(df_stm, cutoff = 0.05)
net <- net$posadj
net <- network::network(net, directed = F)
net %v% "term" = as.character(topic_best_term$term)
net %v% "topic" = as.character(topic_best_term$topic)

y = RColorBrewer::brewer.pal(9, "Set3")
names(y) = levels(as.factor(topic_best_term$term))

ggnet2(net, color = "term", palette = y, alpha = 0.75,
       size = 4, edge.alpha = 0.5,node.label = "topic",
       node.size = 5) +
  labs(title = "Topic Correlations",
       subtitle = "Nodes indicate topics, edges indicate correlation between topics")

#rm(net, topic_best_term)

# plot(prep, "months_since_first_pub",
#      method = "continuous", topics = c(,),
#      model = z, printlegend = F,
#      xaxt = "n", xlab = "Time (1991-2020)")
# monthseq <- seq(from = as.Date("1990-01-01"),
#                 to = as.Date("2021-12-31"), by = "month")
# monthnames <- months(monthseq)
# axis(1,at = as.numeric(monthseq) - min(as.numeric(monthseq)))
# 
```

```{r}
i <- 100
```



```{r out.width = "600px", echo=F}
knitr::include_graphics("images/Screenshot 2020-09-20 at 22.23.52.png")
```


```{r}
df_stm <- k_result %>% 
  filter(K == i) %>% 
  pull(topic_model) %>% 
  .[[1]]

df_stm_td <- tidy(df_stm)

top_terms <- df_stm_td %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>%
  unnest(cols = c(terms))

td_gamma <- tidy(df_stm, matrix = "gamma",
                 document_names = rownames(df_stm))

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

gamma_terms %>%
  top_n(floor(nrow(gamma_terms) * .3), gamma) %>%
  ggplot(aes(topic, gamma, label = terms)) +
  geom_col(aes(color = "black"), alpha = 0.8, show.legend = FALSE, color = "white") +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, gamma_terms %>% pull(gamma) %>% max() * 5),
                     labels = percent_format()) +
  labs(x = NULL, y = expression(gamma),
       title = "Top 20 topics by prevalence",
       subtitle = "With the top words that contribute to each topic") 

# gamma_terms %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

# LDA visualization
toLDAvis(df_stm, docs)

# covariates
out$meta$field = as.factor(out$meta$field)
prep <- estimateEffect(1:i ~ field + s(months_since_first_pub),
                       df_stm,
                       meta = out$meta,
                       uncertainty = "Global")

result_reg <- tidy(prep) %>%
  mutate(p.value = round(p.value, digits = 3),
         term = str_remove(term, "field"))
  
# result_reg %>%
#   mutate(p.value = cell_spec(p.value, "html", color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
#   kable(format = "html", escape = F) %>%
#   kable_classic("striped", full_width = F)

topic_field_matched <- result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate) 

# topic_field_matched %>%
#   mutate(p.value = cell_spec(p.value, "html", color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
#   kable(format = "html", escape = F) %>%
#   kable_classic("striped", full_width = F)

topic_best_term <- result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate)

topic_field_gamma <- gamma_terms %>%
  mutate(topic = as.factor(trimws(str_remove(topic, "Topic ")))) %>%
  left_join(topic_field_matched %>% select(topic, term) %>% mutate(topic = as.factor(topic)) %>% rename(field = term)) %>%
  select(topic, field, gamma, terms) 

# topic_field_gamma %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

a <- labelTopics(df_stm, topics = c(1:i))

topic_full_description <- tibble(Topic = a$topicnums,
       frex = as_tibble(a$frex) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       prob = as_tibble(a$prob) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       lift = as_tibble(a$lift) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       score = as_tibble(a$score) %>% unite(frex, V1:V7, sep = ", ") %>% pull())

topic_full_table <- topic_field_gamma %>%
  left_join(topic_full_description %>% janitor::clean_names() %>% mutate(topic = as.factor(topic))) %>%
  select(-terms)

# topic_full_table %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

short_docs <- meta %>%
    as_tibble() %>%
    select(abstract) %>%
    mutate(abstract = str_sub(abstract, 1, 800))

topic_thoughts <- findThoughts(df_stm, topic = c(1:i), texts = short_docs$abstract, n = 1)

topic_top_docs <- as_tibble(topic_thoughts$index) %>%
  janitor::clean_names() %>%
  pivot_longer(everything()) %>%
  left_join(short_docs %>% mutate(value = row_number())) %>%
  rename(topic = name, doc_index = value, short_abstract = abstract) %>%
  mutate(topic = str_remove(topic, "topic_")) 

table_final <- topic_full_table %>%
  left_join(topic_top_docs) %>%
  select(-doc_index) %>%
  kable() %>%
  kable_classic("striped", full_width = F)
table_final

# network
net <- topicCorr(df_stm, cutoff = 0.05)
net <- net$posadj
net <- network::network(net, directed = F)
net %v% "term" = as.character(topic_best_term$term)
net %v% "topic" = as.character(topic_best_term$topic)

y = RColorBrewer::brewer.pal(9, "Set3")
names(y) = levels(as.factor(topic_best_term$term))

ggnet2(net, color = "term", palette = y, alpha = 0.75,
       size = 4, edge.alpha = 0.5,node.label = "topic",
       node.size = 5) +
  labs(title = "Topic Correlations",
       subtitle = "Nodes indicate topics, edges indicate correlation between topics")

#rm(net, topic_best_term)

# plot(prep, "months_since_first_pub",
#      method = "continuous", topics = c(,),
#      model = z, printlegend = F,
#      xaxt = "n", xlab = "Time (1991-2020)")
# monthseq <- seq(from = as.Date("1990-01-01"),
#                 to = as.Date("2021-12-31"), by = "month")
# monthnames <- months(monthseq)
# axis(1,at = as.numeric(monthseq) - min(as.numeric(monthseq)))
# 
```

```{r}
i <- 120
```


```{r out.width = "600px", echo=F}
knitr::include_graphics("images/Screenshot 2020-09-20 at 22.23.55.png")
```



```{r}
df_stm <- k_result %>% 
  filter(K == i) %>% 
  pull(topic_model) %>% 
  .[[1]]

df_stm_td <- tidy(df_stm)

top_terms <- df_stm_td %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>%
  unnest(cols = c(terms))

td_gamma <- tidy(df_stm, matrix = "gamma",
                 document_names = rownames(df_stm))

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

gamma_terms %>%
  top_n(floor(nrow(gamma_terms) * .3), gamma) %>%
  ggplot(aes(topic, gamma, label = terms)) +
  geom_col(aes(color = "black"), alpha = 0.8, show.legend = FALSE, color = "white") +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, gamma_terms %>% pull(gamma) %>% max() * 5),
                     labels = percent_format()) +
  labs(x = NULL, y = expression(gamma),
       title = "Top 20 topics by prevalence",
       subtitle = "With the top words that contribute to each topic") 

# gamma_terms %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

# LDA visualization
toLDAvis(df_stm, docs)

# covariates
out$meta$field = as.factor(out$meta$field)
prep <- estimateEffect(1:i ~ field + s(months_since_first_pub),
                       df_stm,
                       meta = out$meta,
                       uncertainty = "Global")

result_reg <- tidy(prep) %>%
  mutate(p.value = round(p.value, digits = 3),
         term = str_remove(term, "field"))
  
# result_reg %>%
#   mutate(p.value = cell_spec(p.value, "html", color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
#   kable(format = "html", escape = F) %>%
#   kable_classic("striped", full_width = F)

topic_field_matched <- result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate) 

# topic_field_matched %>%
#   mutate(p.value = cell_spec(p.value, "html", color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
#   kable(format = "html", escape = F) %>%
#   kable_classic("striped", full_width = F)

topic_best_term <- result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate)

topic_field_gamma <- gamma_terms %>%
  mutate(topic = as.factor(trimws(str_remove(topic, "Topic ")))) %>%
  left_join(topic_field_matched %>% select(topic, term) %>% mutate(topic = as.factor(topic)) %>% rename(field = term)) %>%
  select(topic, field, gamma, terms) 

# topic_field_gamma %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

a <- labelTopics(df_stm, topics = c(1:i))

topic_full_description <- tibble(Topic = a$topicnums,
       frex = as_tibble(a$frex) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       prob = as_tibble(a$prob) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       lift = as_tibble(a$lift) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       score = as_tibble(a$score) %>% unite(frex, V1:V7, sep = ", ") %>% pull())

topic_full_table <- topic_field_gamma %>%
  left_join(topic_full_description %>% janitor::clean_names() %>% mutate(topic = as.factor(topic))) %>%
  select(-terms)

# topic_full_table %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

short_docs <- meta %>%
    as_tibble() %>%
    select(abstract) %>%
    mutate(abstract = str_sub(abstract, 1, 800))

topic_thoughts <- findThoughts(df_stm, topic = c(1:i), texts = short_docs$abstract, n = 1)

topic_top_docs <- as_tibble(topic_thoughts$index) %>%
  janitor::clean_names() %>%
  pivot_longer(everything()) %>%
  left_join(short_docs %>% mutate(value = row_number())) %>%
  rename(topic = name, doc_index = value, short_abstract = abstract) %>%
  mutate(topic = str_remove(topic, "topic_")) 

table_final <- topic_full_table %>%
  left_join(topic_top_docs) %>%
  select(-doc_index) %>%
  kable() %>%
  kable_classic("striped", full_width = F)
table_final

# network
net <- topicCorr(df_stm, cutoff = 0.05)
net <- net$posadj
net <- network::network(net, directed = F)
net %v% "term" = as.character(topic_best_term$term)
net %v% "topic" = as.character(topic_best_term$topic)

y = RColorBrewer::brewer.pal(9, "Set3")
names(y) = levels(as.factor(topic_best_term$term))

ggnet2(net, color = "term", palette = y, alpha = 0.75,
       size = 4, edge.alpha = 0.5,node.label = "topic",
       node.size = 5) +
  labs(title = "Topic Correlations",
       subtitle = "Nodes indicate topics, edges indicate correlation between topics")

#rm(net, topic_best_term)

# plot(prep, "months_since_first_pub",
#      method = "continuous", topics = c(,),
#      model = z, printlegend = F,
#      xaxt = "n", xlab = "Time (1991-2020)")
# monthseq <- seq(from = as.Date("1990-01-01"),
#                 to = as.Date("2021-12-31"), by = "month")
# monthnames <- months(monthseq)
# axis(1,at = as.numeric(monthseq) - min(as.numeric(monthseq)))
# 
```

```{r}
i <- 150
```


```{r out.width = "600px", echo=F}
knitr::include_graphics("images/Screenshot 2020-09-20 at 22.23.58.png")
```


```{r}
df_stm <- k_result %>% 
  filter(K == i) %>% 
  pull(topic_model) %>% 
  .[[1]]

df_stm_td <- tidy(df_stm)

top_terms <- df_stm_td %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>%
  unnest(cols = c(terms))

td_gamma <- tidy(df_stm, matrix = "gamma",
                 document_names = rownames(df_stm))

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

gamma_terms %>%
  top_n(floor(nrow(gamma_terms) * .3), gamma) %>%
  ggplot(aes(topic, gamma, label = terms)) +
  geom_col(aes(color = "black"), alpha = 0.8, show.legend = FALSE, color = "white") +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, gamma_terms %>% pull(gamma) %>% max() * 5),
                     labels = percent_format()) +
  labs(x = NULL, y = expression(gamma),
       title = "Top 20 topics by prevalence",
       subtitle = "With the top words that contribute to each topic") 

# gamma_terms %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

# LDA visualization
toLDAvis(df_stm, docs)

# covariates
out$meta$field = as.factor(out$meta$field)
prep <- estimateEffect(1:i ~ field + s(months_since_first_pub),
                       df_stm,
                       meta = out$meta,
                       uncertainty = "Global")

result_reg <- tidy(prep) %>%
  mutate(p.value = round(p.value, digits = 3),
         term = str_remove(term, "field"))
  
# result_reg %>%
#   mutate(p.value = cell_spec(p.value, "html", color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
#   kable(format = "html", escape = F) %>%
#   kable_classic("striped", full_width = F)

topic_field_matched <- result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate) 

# topic_field_matched %>%
#   mutate(p.value = cell_spec(p.value, "html", color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
#   kable(format = "html", escape = F) %>%
#   kable_classic("striped", full_width = F)

topic_best_term <- result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate)

topic_field_gamma <- gamma_terms %>%
  mutate(topic = as.factor(trimws(str_remove(topic, "Topic ")))) %>%
  left_join(topic_field_matched %>% select(topic, term) %>% mutate(topic = as.factor(topic)) %>% rename(field = term)) %>%
  select(topic, field, gamma, terms) 

# topic_field_gamma %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

a <- labelTopics(df_stm, topics = c(1:i))

topic_full_description <- tibble(Topic = a$topicnums,
       frex = as_tibble(a$frex) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       prob = as_tibble(a$prob) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       lift = as_tibble(a$lift) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       score = as_tibble(a$score) %>% unite(frex, V1:V7, sep = ", ") %>% pull())

topic_full_table <- topic_field_gamma %>%
  left_join(topic_full_description %>% janitor::clean_names() %>% mutate(topic = as.factor(topic))) %>%
  select(-terms)

# topic_full_table %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

short_docs <- meta %>%
    as_tibble() %>%
    select(abstract) %>%
    mutate(abstract = str_sub(abstract, 1, 800))

topic_thoughts <- findThoughts(df_stm, topic = c(1:i), texts = short_docs$abstract, n = 1)

topic_top_docs <- as_tibble(topic_thoughts$index) %>%
  janitor::clean_names() %>%
  pivot_longer(everything()) %>%
  left_join(short_docs %>% mutate(value = row_number())) %>%
  rename(topic = name, doc_index = value, short_abstract = abstract) %>%
  mutate(topic = str_remove(topic, "topic_")) 

table_final <- topic_full_table %>%
  left_join(topic_top_docs) %>%
  select(-doc_index) %>%
  kable() %>%
  kable_classic("striped", full_width = F)
table_final

# network
net <- topicCorr(df_stm, cutoff = 0.05)
net <- net$posadj
net <- network::network(net, directed = F)
net %v% "term" = as.character(topic_best_term$term)
net %v% "topic" = as.character(topic_best_term$topic)

y = RColorBrewer::brewer.pal(9, "Set3")
names(y) = levels(as.factor(topic_best_term$term))

ggnet2(net, color = "term", palette = y, alpha = 0.75,
       size = 4, edge.alpha = 0.5,node.label = "topic",
       node.size = 5) +
  labs(title = "Topic Correlations",
       subtitle = "Nodes indicate topics, edges indicate correlation between topics")

#rm(net, topic_best_term)

# plot(prep, "months_since_first_pub",
#      method = "continuous", topics = c(,),
#      model = z, printlegend = F,
#      xaxt = "n", xlab = "Time (1991-2020)")
# monthseq <- seq(from = as.Date("1990-01-01"),
#                 to = as.Date("2021-12-31"), by = "month")
# monthnames <- months(monthseq)
# axis(1,at = as.numeric(monthseq) - min(as.numeric(monthseq)))
# 
```


```{r}
i <- 200
```


```{r out.width = "600px", echo=F}
knitr::include_graphics("images/Screenshot 2020-09-20 at 22.28.48.png")
```


```{r}
df_stm <- k_result %>% 
  filter(K == i) %>% 
  pull(topic_model) %>% 
  .[[1]]

df_stm_td <- tidy(df_stm)

top_terms <- df_stm_td %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>%
  unnest(cols = c(terms))

td_gamma <- tidy(df_stm, matrix = "gamma",
                 document_names = rownames(df_stm))

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

gamma_terms %>%
  top_n(floor(nrow(gamma_terms) * .3), gamma) %>%
  ggplot(aes(topic, gamma, label = terms)) +
  geom_col(aes(color = "black"), alpha = 0.8, show.legend = FALSE, color = "white") +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, gamma_terms %>% pull(gamma) %>% max() * 5),
                     labels = percent_format()) +
  labs(x = NULL, y = expression(gamma),
       title = "Top 20 topics by prevalence",
       subtitle = "With the top words that contribute to each topic") 

# gamma_terms %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

# LDA visualization
toLDAvis(df_stm, docs)

# covariates
out$meta$field = as.factor(out$meta$field)
prep <- estimateEffect(1:i ~ field + s(months_since_first_pub),
                       df_stm,
                       meta = out$meta,
                       uncertainty = "Global")

result_reg <- tidy(prep) %>%
  mutate(p.value = round(p.value, digits = 3),
         term = str_remove(term, "field"))
  
# result_reg %>%
#   mutate(p.value = cell_spec(p.value, "html", color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
#   kable(format = "html", escape = F) %>%
#   kable_classic("striped", full_width = F)

topic_field_matched <- result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate) 

# topic_field_matched %>%
#   mutate(p.value = cell_spec(p.value, "html", color = ifelse(p.value <= 0.05, "blue", "red"))) %>%
#   kable(format = "html", escape = F) %>%
#   kable_classic("striped", full_width = F)

topic_best_term <- result_reg %>%
  filter(!str_detect(term, pattern = "s[(]"),
         p.value < 0.1) %>%
  mutate(term = ifelse(term == "(Intercept)", "Business", term),
         term = ifelse(estimate < 0, "Unclear", term)) %>%
  group_by(topic) %>%
  mutate(max_estimate = max(estimate)) %>%
  ungroup() %>%
  filter(estimate == max_estimate)

topic_field_gamma <- gamma_terms %>%
  mutate(topic = as.factor(trimws(str_remove(topic, "Topic ")))) %>%
  left_join(topic_field_matched %>% select(topic, term) %>% mutate(topic = as.factor(topic)) %>% rename(field = term)) %>%
  select(topic, field, gamma, terms) 

# topic_field_gamma %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

a <- labelTopics(df_stm, topics = c(1:i))

topic_full_description <- tibble(Topic = a$topicnums,
       frex = as_tibble(a$frex) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       prob = as_tibble(a$prob) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       lift = as_tibble(a$lift) %>% unite(frex, V1:V7, sep = ", ") %>% pull(),
       score = as_tibble(a$score) %>% unite(frex, V1:V7, sep = ", ") %>% pull())

topic_full_table <- topic_field_gamma %>%
  left_join(topic_full_description %>% janitor::clean_names() %>% mutate(topic = as.factor(topic))) %>%
  select(-terms)

# topic_full_table %>%
#   kable() %>%
#   kable_classic("striped", full_width = F)

short_docs <- meta %>%
    as_tibble() %>%
    select(abstract) %>%
    mutate(abstract = str_sub(abstract, 1, 800))

topic_thoughts <- findThoughts(df_stm, topic = c(1:i), texts = short_docs$abstract, n = 1)

topic_top_docs <- as_tibble(topic_thoughts$index) %>%
  janitor::clean_names() %>%
  pivot_longer(everything()) %>%
  left_join(short_docs %>% mutate(value = row_number())) %>%
  rename(topic = name, doc_index = value, short_abstract = abstract) %>%
  mutate(topic = str_remove(topic, "topic_")) 

table_final <- topic_full_table %>%
  left_join(topic_top_docs) %>%
  select(-doc_index) %>%
  kable() %>%
  kable_classic("striped", full_width = F)
table_final

# network
net <- topicCorr(df_stm, cutoff = 0.05)
net <- net$posadj
net <- network::network(net, directed = F)
net %v% "term" = as.character(topic_best_term$term)
net %v% "topic" = as.character(topic_best_term$topic)

y = RColorBrewer::brewer.pal(9, "Set3")
names(y) = levels(as.factor(topic_best_term$term))

ggnet2(net, color = "term", palette = y, alpha = 0.75,
       size = 4, edge.alpha = 0.5,node.label = "topic",
       node.size = 5) +
  labs(title = "Topic Correlations",
       subtitle = "Nodes indicate topics, edges indicate correlation between topics")

#rm(net, topic_best_term)

# plot(prep, "months_since_first_pub",
#      method = "continuous", topics = c(,),
#      model = z, printlegend = F,
#      xaxt = "n", xlab = "Time (1991-2020)")
# monthseq <- seq(from = as.Date("1990-01-01"),
#                 to = as.Date("2021-12-31"), by = "month")
# monthnames <- months(monthseq)
# axis(1,at = as.numeric(monthseq) - min(as.numeric(monthseq)))
# 
```


</body>